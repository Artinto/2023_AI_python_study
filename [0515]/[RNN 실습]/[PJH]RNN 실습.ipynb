{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7449a1cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 4.6231, Accuracy: 0.00%, Predicted String: /i/i0y//i/\n",
      "Epoch [2/15], Loss: 3.7210, Accuracy: 40.00%, Predicted String: llllloolll\n",
      "Epoch [3/15], Loss: 2.2563, Accuracy: 40.00%, Predicted String: lllloooooo\n",
      "Epoch [4/15], Loss: 1.8349, Accuracy: 30.00%, Predicted String: llllllllll\n",
      "Epoch [5/15], Loss: 1.6239, Accuracy: 40.00%, Predicted String: llllloolll\n",
      "Epoch [6/15], Loss: 1.4432, Accuracy: 40.00%, Predicted String: hlllllllll\n",
      "Epoch [7/15], Loss: 1.3538, Accuracy: 50.00%, Predicted String: helooooooo\n",
      "Epoch [8/15], Loss: 1.1346, Accuracy: 60.00%, Predicted String: helloooooo\n",
      "Epoch [9/15], Loss: 1.0355, Accuracy: 50.00%, Predicted String: helllorlll\n",
      "Epoch [10/15], Loss: 0.9447, Accuracy: 50.00%, Predicted String: helllollll\n",
      "Epoch [11/15], Loss: 0.8768, Accuracy: 80.00%, Predicted String: hellowolow\n",
      "Epoch [12/15], Loss: 0.8003, Accuracy: 80.00%, Predicted String: hellowoloo\n",
      "Epoch [13/15], Loss: 0.6656, Accuracy: 70.00%, Predicted String: hellooollo\n",
      "Epoch [14/15], Loss: 0.6171, Accuracy: 70.00%, Predicted String: hellooollo\n",
      "Epoch [15/15], Loss: 0.5533, Accuracy: 90.00%, Predicted String: hellooolrd\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import string\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "# string.pritable을 이용하여 출력가능한 아스키 문자를 담은 문자열 상수\n",
    "input_size = len(string.printable) \n",
    "hidden_size = 128\n",
    "output_size = len(string.printable)\n",
    "num_layers = 2\n",
    "learning_rate = 0.01\n",
    "num_epochs = 15\n",
    "\n",
    "# 훈련 데이터 정의\n",
    "input_str = 'ehlloowlrd'\n",
    "output_str = 'hellowolrd'\n",
    "\n",
    "all_chars = string.printable\n",
    "\n",
    "# 문자를 인덱스로 변환\n",
    "char2index = dict(zip(all_chars, range(len(all_chars))))\n",
    "# 인덱스를 문자로 변환\n",
    "index2char = dict(zip(range(len(all_chars)), all_chars)) \n",
    "\n",
    "# RNN 모델 정의\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # 배치차원이 맨 앞으로 오도록 함\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True) \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # RNN 순전파 수행\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        # out의 크기를 (batch_size * sequence_length, hidden_size)로 변경\n",
    "        out = out.reshape(out.size(0)*out.size(1), self.hidden_size)\n",
    "        # out을 딥러닝 모델에 통과시켜 출력값 계산\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "# 모델 인스턴스 생성\n",
    "model = RNN(input_size, hidden_size, output_size, num_layers)\n",
    "\n",
    "# 손실 함수와 최적화 알고리즘 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 훈련 루프\n",
    "for epoch in range(num_epochs):\n",
    "    # 각 시점에서 hidden state가 유지되어야 하기 때문에 초기화를 위해 None 할당\n",
    "    hidden = None\n",
    "    \n",
    "    # 입력과 출력 데이터 생성\n",
    "    input_tensor = torch.tensor([[char2index[c] for c in input_str]])\n",
    "    output_tensor = torch.tensor([char2index[c] for c in output_str])\n",
    "    \n",
    "    # 모델 입력을 위한 one-hot encoding\n",
    "    input_one_hot = nn.functional.one_hot(input_tensor, num_classes=len(all_chars)).float()\n",
    "    \n",
    "    # 순전파와 역전파\n",
    "    optimizer.zero_grad()\n",
    "    output, hidden = model(input_one_hot, hidden)\n",
    "    loss = criterion(output, output_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 예측값 계산\n",
    "    # 모델이 예측한 값 중 확률이 가장 높은 문자의 인덱스를 가져옴\n",
    "    output_indices = output.argmax(dim=1)\n",
    "    # output_indices의 값들을 하나씩 가져와 다시 문자형태로 변환 후 공백없이 문자열로 합쳐준다\n",
    "    predicted_chars = ''.join([index2char[i.item()] for i in output_indices])\n",
    "    \n",
    "    # 정확도 계산\n",
    "    accuracy = torch.mean((output_indices == output_tensor).float())\n",
    "    \n",
    "    # loss와 accuracy 출력\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy.item()*100:.2f}%, Predicted String: {predicted_chars}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c671f431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력데이터 :  elworoldlh\n",
      "출력데이터 :  hellowolrd\n",
      "Epoch [1/20], Loss: 4.5982, Accuracy: 0.00%, Predicted String: .TBTTBBBBT\n",
      "Epoch [2/20], Loss: 3.4539, Accuracy: 50.00%, Predicted String: olllololll\n",
      "Epoch [3/20], Loss: 2.1375, Accuracy: 30.00%, Predicted String: olllllllll\n",
      "Epoch [4/20], Loss: 1.7844, Accuracy: 30.00%, Predicted String: llllllllll\n",
      "Epoch [5/20], Loss: 1.5765, Accuracy: 40.00%, Predicted String: hllooooooo\n",
      "Epoch [6/20], Loss: 1.3115, Accuracy: 60.00%, Predicted String: hellloolol\n",
      "Epoch [7/20], Loss: 4.0802, Accuracy: 50.00%, Predicted String: hellloo4$v\n",
      "Epoch [8/20], Loss: 1.6255, Accuracy: 40.00%, Predicted String: heellllloo\n",
      "Epoch [9/20], Loss: 1.6184, Accuracy: 40.00%, Predicted String: heelllllll\n",
      "Epoch [10/20], Loss: 1.3175, Accuracy: 60.00%, Predicted String: helloooooo\n",
      "Epoch [11/20], Loss: 1.2191, Accuracy: 50.00%, Predicted String: helooooooo\n",
      "Epoch [12/20], Loss: 1.0890, Accuracy: 60.00%, Predicted String: helloooooo\n",
      "Epoch [13/20], Loss: 1.1168, Accuracy: 50.00%, Predicted String: hellllllll\n",
      "Epoch [14/20], Loss: 1.1071, Accuracy: 60.00%, Predicted String: hellrlrlrl\n",
      "Epoch [15/20], Loss: 1.0854, Accuracy: 50.00%, Predicted String: hlllrlrlrl\n",
      "Epoch [16/20], Loss: 0.9663, Accuracy: 60.00%, Predicted String: hellrlrlrl\n",
      "Epoch [17/20], Loss: 0.8584, Accuracy: 70.00%, Predicted String: hellololol\n",
      "Epoch [18/20], Loss: 0.8334, Accuracy: 70.00%, Predicted String: hellololoo\n",
      "Epoch [19/20], Loss: 0.7687, Accuracy: 70.00%, Predicted String: hellololoo\n",
      "Epoch [20/20], Loss: 0.7577, Accuracy: 70.00%, Predicted String: hellololol\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import string\n",
    "import random\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "# string.pritable을 이용하여 출력가능한 아스키 문자를 담은 문자열 상수\n",
    "input_size = len(string.printable) \n",
    "hidden_size = 128\n",
    "output_size = len(string.printable)\n",
    "num_layers = 4\n",
    "learning_rate = 0.01\n",
    "num_epochs = 20\n",
    "\n",
    "# 훈련 데이터 정의\n",
    "input_str = 'hellowolrd'\n",
    "output_str = 'hellowolrd'\n",
    "\n",
    "# input_str과 output_str을 리스트로 변환\n",
    "input_list = list(input_str)\n",
    "\n",
    "# 리스트를 랜덤하게 섞음\n",
    "random.shuffle(input_list)\n",
    "\n",
    "# 섞인 리스트를 다시 문자열로 변환\n",
    "shuffled_input_str = ''.join(input_list)\n",
    "\n",
    "print('입력데이터 : ', shuffled_input_str)  # 예시 출력: \"dheoowlllr\"\n",
    "print('출력데이터 : ', output_str)\n",
    "\n",
    "all_chars = string.printable\n",
    "\n",
    "# 문자를 인덱스로 변환\n",
    "char2index = dict(zip(all_chars, range(len(all_chars))))\n",
    "# 인덱스를 문자로 변환\n",
    "index2char = dict(zip(range(len(all_chars)), all_chars)) \n",
    "\n",
    "# RNN 모델 정의\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # 배치차원이 맨 앞으로 오도록 함\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True) \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # RNN 순전파 수행\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        # out의 크기를 (batch_size * sequence_length, hidden_size)로 변경\n",
    "        out = out.reshape(out.size(0)*out.size(1), self.hidden_size)\n",
    "        # out을 딥러닝 모델에 통과시켜 출력값 계산\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "# 모델 인스턴스 생성\n",
    "model = RNN(input_size, hidden_size, output_size, num_layers)\n",
    "\n",
    "# 손실 함수와 최적화 알고리즘 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 훈련 루프\n",
    "for epoch in range(num_epochs):\n",
    "    # 각 시점에서 hidden state가 유지되어야 하기 때문에 초기화를 위해 None 할당\n",
    "    hidden = None\n",
    "    \n",
    "    # 입력과 출력 데이터 생성\n",
    "    input_tensor = torch.tensor([[char2index[c] for c in input_str]])\n",
    "    output_tensor = torch.tensor([char2index[c] for c in output_str])\n",
    "    \n",
    "    # 모델 입력을 위한 one-hot encoding\n",
    "    input_one_hot = nn.functional.one_hot(input_tensor, num_classes=len(all_chars)).float()\n",
    "    \n",
    "    # 순전파와 역전파\n",
    "    optimizer.zero_grad()\n",
    "    output, hidden = model(input_one_hot, hidden)\n",
    "    loss = criterion(output, output_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 예측값 계산\n",
    "    # 모델이 예측한 값 중 확률이 가장 높은 문자의 인덱스를 가져옴\n",
    "    output_indices = output.argmax(dim=1)\n",
    "    # output_indices의 값들을 하나씩 가져와 다시 문자형태로 변환 후 공백없이 문자열로 합쳐준다\n",
    "    predicted_chars = ''.join([index2char[i.item()] for i in output_indices])\n",
    "    \n",
    "    # 정확도 계산\n",
    "    accuracy = torch.mean((output_indices == output_tensor).float())\n",
    "    \n",
    "    # loss와 accuracy 출력\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy.item()*100:.2f}%, Predicted String: {predicted_chars}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b769f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력데이터 :  lhodelowrl\n",
      "출력데이터 :  hellowolrd\n",
      "Epoch [1/20], Loss: 4.5871, Accuracy: 0.00%, Predicted String: !P!Z44ZPZ4\n",
      "Epoch [2/20], Loss: 3.5298, Accuracy: 40.00%, Predicted String: hlllllllll\n",
      "Epoch [3/20], Loss: 2.1342, Accuracy: 30.00%, Predicted String: llllllllll\n",
      "Epoch [4/20], Loss: 1.7328, Accuracy: 30.00%, Predicted String: llllllllll\n",
      "Epoch [5/20], Loss: 1.4476, Accuracy: 60.00%, Predicted String: holloooloo\n",
      "Epoch [6/20], Loss: 1.1402, Accuracy: 50.00%, Predicted String: hellllllll\n",
      "Epoch [7/20], Loss: 0.8413, Accuracy: 80.00%, Predicted String: hellololod\n",
      "Epoch [8/20], Loss: 7.0093, Accuracy: 30.00%, Predicted String: hepFheoqFp\n",
      "Epoch [9/20], Loss: 3.2131, Accuracy: 60.00%, Predicted String: hellooo#qT\n",
      "Epoch [10/20], Loss: 1.1847, Accuracy: 70.00%, Predicted String: helllwoloo\n",
      "Epoch [11/20], Loss: 0.9731, Accuracy: 70.00%, Predicted String: helllwoloo\n",
      "Epoch [12/20], Loss: 0.7727, Accuracy: 80.00%, Predicted String: hellowoloo\n",
      "Epoch [13/20], Loss: 0.6044, Accuracy: 80.00%, Predicted String: hellowoloo\n",
      "Epoch [14/20], Loss: 0.5193, Accuracy: 90.00%, Predicted String: hellowolro\n",
      "Epoch [15/20], Loss: 0.4271, Accuracy: 100.00%, Predicted String: hellowolrd\n",
      "Epoch [16/20], Loss: 0.3404, Accuracy: 100.00%, Predicted String: hellowolrd\n",
      "Epoch [17/20], Loss: 0.2608, Accuracy: 100.00%, Predicted String: hellowolrd\n",
      "Epoch [18/20], Loss: 0.1989, Accuracy: 100.00%, Predicted String: hellowolrd\n",
      "Epoch [19/20], Loss: 0.1507, Accuracy: 100.00%, Predicted String: hellowolrd\n",
      "Epoch [20/20], Loss: 0.1161, Accuracy: 100.00%, Predicted String: hellowolrd\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import string\n",
    "import random\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "input_size = len(string.printable) \n",
    "hidden_size = 128\n",
    "output_size = len(string.printable)\n",
    "num_layers = 4\n",
    "learning_rate = 0.01\n",
    "num_epochs = 20\n",
    "\n",
    "# 훈련 데이터 정의\n",
    "input_str = 'hellowolrd'\n",
    "output_str = 'hellowolrd'\n",
    "\n",
    "# input_str과 output_str을 리스트로 변환\n",
    "input_list = list(input_str)\n",
    "\n",
    "# 리스트를 랜덤하게 섞음\n",
    "random.shuffle(input_list)\n",
    "\n",
    "# 섞인 리스트를 다시 문자열로 변환\n",
    "shuffled_input_str = ''.join(input_list)\n",
    "\n",
    "print('입력데이터 : ', shuffled_input_str)\n",
    "print('출력데이터 : ', output_str)\n",
    "\n",
    "all_chars = string.printable\n",
    "\n",
    "# 문자를 인덱스로 변환\n",
    "char2index = dict(zip(all_chars, range(len(all_chars))))\n",
    "# 인덱스를 문자로 변환\n",
    "index2char = dict(zip(range(len(all_chars)), all_chars)) \n",
    "\n",
    "# RNN 모델 정의\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True) \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = out.reshape(out.size(0)*out.size(1), self.hidden_size)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "# 모델 인스턴스 생성\n",
    "model = RNN(input_size, hidden_size, output_size, num_layers)\n",
    "\n",
    "# 손실 함수와 최적화 알고리즘 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# model.parameters() : 모델의 파라미터, lr : 러닝레이트, betas=(0.9, 0.999) : 평균제곱과 제곱근, eps : 숫자 안정성을 위한 값, weight_decay : 가중치 감소\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
    "\n",
    "# 훈련 루프\n",
    "for epoch in range(num_epochs):\n",
    "    hidden = None\n",
    "    \n",
    "    input_tensor = torch.tensor([[char2index[c] for c in input_str]])\n",
    "    output_tensor = torch.tensor([char2index[c] for c in output_str])\n",
    "    \n",
    "    input_one_hot = nn.functional.one_hot(input_tensor, num_classes=len(all_chars)).float()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output, hidden = model(input_one_hot, hidden)\n",
    "    loss = criterion(output, output_tensor)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Adam+ 적용\n",
    "    # 현재 파라미터 값을 이전 값에서 lr * 0.01만큼 감소 시켜 파라미터 업데이트\n",
    "    for p in model.parameters():\n",
    "        p.data.mul_(1.0 - learning_rate * 0.01)\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    output_indices = output.argmax(dim=1)\n",
    "    predicted_chars = ''.join([index2char[i.item()] for i in output_indices])\n",
    "    accuracy = torch.mean((output_indices == output_tensor).float())\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy.item()*100:.2f}%, Predicted String: {predicted_chars}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
