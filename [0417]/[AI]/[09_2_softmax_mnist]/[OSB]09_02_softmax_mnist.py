# -*- coding: utf-8 -*-
"""09_02_softmax_mnist.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c3QSaBTkPBRejhDPPqSr95vOO-iBnNOY
"""

# https://github.com/pytorch/examples/blob/master/mnist/main.py
from __future__ import print_function # Print문을 파이썬 버전3(우리가 아는 방식)를 파이썬 버전 2에서 사용할 수 있도록한다.
from torch import nn, optim, cuda # 신경망 모듈과 optimizer, GPU연산을 위해 import
from torch.utils import data # 파이토치에서 데이터셋을 load하는 모듈을 import, 즉, DataLoader를 사용하기 위해 import
from torchvision import datasets, transforms # MNIST, CIFAR10과 같이 pytorch에서 제공하는 데이터셋 모듈과 데이터 전처리를 위한 transforms import
import torch.nn.functional as F # 활성화 함수(여기엔 ReLU)를 이용하기 위해서 import
import time # 시간을 다루기 위해서 import

# Training settings
batch_size = 64 # mini-batch 단위로 데이터 64개를 한 번에 모델에 입력.
device = 'cuda' if cuda.is_available() else 'cpu' # GPU가 사용가능하면, GPU 사용 아니면 CPU사용.
print(f'Training MNIST Model on {device}\n{"=" * 44}') # 현재 GPU, CPU를 사용하는지 알려주고 출력을 '='을 이용해 분리.

# MNIST Dataset 
# 학습 데이터 셋 생성. torchvision에 존재하는 MNIST 데이터 셋을 다운
train_dataset = datasets.MNIST(root='./mnist_data/', # 데이터 셋을 저장할 경로 지정.
                               train=True,           # 데이터 셋을 학습용과 테스트용으로 분리하는 용도로 train =True 면 학습용을 가져온다.
                               transform=transforms.ToTensor(), # 데이터 셋을 저장할 때 Tensor로 변환(0~1사이의 실수형으로 정규화)해서 저장한다.
                               download=True) # 현재 지정된 root에 데이터셋이 없다면 다운로드, 있다면 다운로드하지 않음.
# 테스트 데이터 셋 생성. torchvision에 존재하는 MNIST 데이터 셋을 다운
test_dataset = datasets.MNIST(root='./mnist_data/', # 데이터셋을 저장할 경로
                              train=False, # 데이터 셋을 테스트용을 저장하겠다.
                              transform=transforms.ToTensor())

# Data Loader (Input Pipeline)
# 앞서 저장한 train_dataset과 test_dataset(텐서로 구성)을 앞서 선언한 batch_size로 불러온다.
train_loader = data.DataLoader(dataset=train_dataset,
                                           batch_size=batch_size,
                                           shuffle=True) # train의 경우 과적합을 방지하기 위해서 shuffle을 해줌.

test_loader = data.DataLoader(dataset=test_dataset,
                                          batch_size=batch_size,
                                          shuffle=False) # test의 경우 결과를 출력해야하므로 shufffle을 따로 해줄 필요가 없음.


class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__() 
        self.l1 = nn.Linear(784, 520) # 784 -> 520 선형 layer
        self.l2 = nn.Linear(520, 320) # 520 -> 320 선형 layer
        self.l3 = nn.Linear(320, 240) # 320 -> 240 선형 layer
        self.l4 = nn.Linear(240, 120) # 240 -> 120 선형 layer
        self.l5 = nn.Linear(120, 10)  # 120 -> 10  선형 layer

    def forward(self, x): # x값을 받아 처리 후 최종적으로 10개의 값을 반환
        x = x.view(-1, 784)  # Flatten the data (n, 1, 28, 28)-> (n, 784)
        x = F.relu(self.l1(x))  # 첫번째 fully connected layer에 activation 함수로 relu적용
        x = F.relu(self.l2(x))  # 두번째 fully connected layer에 activation 함수로 relu적용
        x = F.relu(self.l3(x))  # 세번째 fully connected layer에 activation 함수로 relu적용
        x = F.relu(self.l4(x))  # 네번째 fully connected layer에activation 함수로 relu적용
        return self.l5(x) # 마지막의 fully connected 레이어를 거지고 최종 출력값 반환(64, 10)의 크기로 반환됨.
                          # nn.CrossEntropy를 사용하기 때문에 softmax 단계가 없음.


model = Net() # 모델 생성
model.to(device) # 위에서 지정된 CPU or GPU로 사용/
criterion = nn.CrossEntropyLoss() # cost-function으로 CrossEntropyLoss()를 사용.
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5) # Stochastic Gradient Descent를 optimizer로 생성하고, 학습률은 0.01로 한다.
# momentum을 이용해 이전의 SGD 업데이트 방향성을 반영한다. 보통 0.5~ 0.9로 반영(클수록 더 많이 반영한다.)

# 학습하는 함수 정의(인자로 epoch(학습할 수)를 받는다.)
def train(epoch):
    model.train() # model을 학습모드로 설정해 학습된것을 반영하도록함.
    for batch_idx, (data, target) in enumerate(train_loader): # X값과 라벨 값을 train_loader가 가지고 있으므로 이를 data와 target으로 선언받고, enumerate를 이용해서 batch의 인덱스도 받을수 있도록 한다.
        data, target = data.to(device), target.to(device) # 데이터와 타겟을 설정한 CPU or GPU로 이동한.
        optimizer.zero_grad() # Optimizer의 gradient를 0으로 설정(초기화), 이전 batch 의 gradient가 현재 batch에 영향받는 것을 방지함.
        output = model(data) # 모델을 통해서 forward 과정을 진행함.(예측)
        loss = criterion(output, target) # 예측한 값과 실제 값의 손실을 cross entropy loss을 이용해 계산한다
        loss.backward() # backward을 진행
        optimizer.step() # backward의 결과를 이용해 optimizer를 weight를 업데이트한다.
        if batch_idx % 10 == 0: # 학습하는 과정이 10번째마다 배치 상태와 손실를 출력한다.
            print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.item()))

# 모델을 평가하는 함수 정의.
def test():
    model.eval() # model을 평가모드로 설정한다.
    test_loss = 0 # 초기의 loss 지정
    correct = 0 # 초기의 correct 지정.
    for data, target in test_loader: # test에서는 배치의 index를 알 필요가 없으므로 사용하지 않음.
        data, target = data.to(device), target.to(device) # 데이터와 타켓을 설정한 CPU or GPU로 이동한다.
        output = model(data) # 학습한 모델에 data(X)를 넣어서 예측값을 저장함.
        # sum up batch loss
        test_loss += criterion(output, target).item() # .item을 통해 Tensor로 반환된 오차 값을 값만 사용하고 이를 누적한다.
        # get the index of the max
        # max함수의 경우 예측값의 각 행마다 가장 큰 값을 가지는 값과 그 행에서의 index 값을 반환한다. axis =1로 최대값을 구하고 keepdim=True로 최대값을 구한 축의 차원을 유지한다..
        pred = output.data.max(1, keepdim=True)[1] # 해당 경우 결과 shape이 (64(batch_size), 1)인 텐서이지만 indexing을 통해 해당 큰 값을 가지고 있는 index만 pred로 저장한다.
        correct += pred.eq(target.data.view_as(pred)).cpu().sum() # 위에서 pred에 저장되어 있는 값은 결국 0~9사이의 예상되는 mnist 값중 하나이므로 이를 실제 데이터인 target과 비교하여 맞으면 1 아니면 0의 Tensor로 반환함.
        # cpu().sum()을 통해서 반환된 Tensor의 값들을 CPU상에 모두 더한다. 이를 통해 Correct에는 전체 배치에서 예상값 = 실제값인 샘플의 수를 저장한다.

    test_loss /= len(test_loader.dataset) # 전체 배치로 나눠서 평균 손실값으로 바꿈.
    # Test set의 평균 손실값과 정확도를 나타냄. 정확도의 경우 위에서 Correct에 저잗된것은 맞은 샘플의 수 이므로 이를 전체 batch에 대해 나눠 정확도로 바꿔준다.
    print(f'===========================\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '
          f'({100. * correct / len(test_loader.dataset):.0f}%)')


if __name__ == '__main__': # 현재 저장된 py파일이 import형식으로 실행되지 않고 인터프리터로 실행되는 경우 실행한다.
    since = time.time() # 학습시간을 계산하기 위해서 해당 줄이 시작되는 시점을 since에 저장ㅎ나다.
    for epoch in range(1, 10): # 총 9번의 epoch
        epoch_start = time.time() # since와 동일하게 학습시간을 보기 위해서 사용함.
        train(epoch) # train의 출력결과로 어느 epoch인지 알려주는 print문이 있으므로 epoch가 인자로 들어가며, 모델을 학습하는 함수를 호출.
        m, s = divmod(time.time() - epoch_start, 60) # divmod의 경우 몫과 나머지를 출력하는데 time()의 단위가 초 단위라 이를 분과 초 단위로 변환해서 저장한다.
        print(f'Training time: {m:.0f}m {s:.0f}s') # 학습에 걸린 시간을 출력한다.
        test() # 모델을 평가하는 함수 호출.
        m, s = divmod(time.time() - epoch_start, 60) # 위와 동일
        print(f'Testing time: {m:.0f}m {s:.0f}s') # 학습과 평가에 걸린 시간을 출력한다.

    m, s = divmod(time.time() - since, 60) # 모델 학습 및 평가에 걸린 총 시간을 저장.
    print(f'Total Time: {m:.0f}m {s:.0f}s\nModel was trained on {device}!') # 총 학습 시간 및 사용한 device(CPU or GPU)를 출력함